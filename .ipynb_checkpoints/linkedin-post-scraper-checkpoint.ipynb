{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Initialize Chrome options\n",
    "chrome_options = Options()\n",
    "\n",
    "# Set up date formatting for today's date\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url of the Linkedin Page you want to scrape\n",
    "page = 'https://www.linkedin.com/company/airesearchcentre/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into LinkedIn\n"
     ]
    }
   ],
   "source": [
    "#access Webriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "email = \"woxsenailab@gmail.com\"\n",
    "password = \"Ai@l@bfaculty@2024\"\n",
    "csv_file = \"social_posts.csv\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "browser = webdriver.Chrome(options=options)\n",
    "        \n",
    "    \n",
    "print(\"Logging into LinkedIn\")\n",
    "browser.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "# Wait for login elements to load\n",
    "wait = WebDriverWait(browser, 10)\n",
    "username = wait.until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "username.send_keys(email)\n",
    "\n",
    "pword = browser.find_element(By.ID, \"password\")\n",
    "pword.send_keys(password)\n",
    "browser.find_element(By.XPATH, \"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to webpage\n",
    "browser.get(\"https://www.linkedin.com/feed/update/urn:li:activity:7353310347857969153/?collapsed=1n\")\n",
    "html = browser.page_source\n",
    "soup =bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = soup.find_all(\"div\", class_=\"fie-impression-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.linkedin.com/feed/update/urn:li:activity:7353310347857969153?collapsed=1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Scraping complete. HTML output saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinkedin_posts.html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 103\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 99\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError scraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m---> 99\u001b[0m \u001b[43mgenerate_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_posts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Scraping complete. HTML output saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinkedin_posts.html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 75\u001b[0m, in \u001b[0;36mgenerate_html\u001b[1;34m(posts, output_file)\u001b[0m\n\u001b[0;32m     73\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<p><strong>Time:</strong> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<p><strong>Reactions:</strong> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<p><strong>Content:</strong><br>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpost\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m),\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<br>\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     77\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<img src=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax-width:400px\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m><br>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    return webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "\n",
    "def extract_post_details(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # wait for JS to load\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    post_data = {\n",
    "        \"url\": url,\n",
    "        \"author\": None,\n",
    "        \"content\": None,\n",
    "        \"image\": None,\n",
    "        \"timestamp\": None,\n",
    "        \"reactions\": None\n",
    "    }\n",
    "\n",
    "    # Try multiple fallback selectors as LinkedIn frequently changes class names\n",
    "    try:\n",
    "        post_data[\"author\"] = soup.find(\"span\", class_=\"feed-shared-actor__name\").get_text(strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        content_div = soup.find(\"div\", {\"class\": lambda x: x and \"update-components-text\" in x})\n",
    "        if content_div:\n",
    "            post_data[\"content\"] = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        img_tag = soup.find(\"img\", {\"src\": True, \"class\": lambda x: x and \"update-components-image\" in x})\n",
    "        if img_tag:\n",
    "            post_data[\"image\"] = img_tag[\"src\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        time_tag = soup.find(\"span\", class_=\"visually-hidden\")\n",
    "        if time_tag:\n",
    "            post_data[\"timestamp\"] = time_tag.get_text(strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        reactions_span = soup.find(\"span\", class_=\"social-details-social-counts__reactions-count\")\n",
    "        post_data[\"reactions\"] = reactions_span.get_text(strip=True) if reactions_span else None\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return post_data\n",
    "\n",
    "def generate_html(posts, output_file=\"linkedin_posts.html\"):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"<html><head><title>LinkedIn Posts</title></head><body>\")\n",
    "        for post in posts:\n",
    "            f.write(f\"<div style='border:1px solid #ccc;padding:10px;margin:10px'>\")\n",
    "            f.write(f\"<p><strong>Author:</strong> {post['author']}</p>\")\n",
    "            f.write(f\"<p><strong>Time:</strong> {post['timestamp']}</p>\")\n",
    "            f.write(f\"<p><strong>Reactions:</strong> {post['reactions']}</p>\")\n",
    "            f.write(f\"<p><strong>Content:</strong><br>{post['content']}</p>\")\n",
    "            if post[\"image\"]:\n",
    "                f.write(f\"<img src='{post['image']}' style='max-width:400px'><br>\")\n",
    "            f.write(f\"<a href='{post['url']}' target='_blank'>View on LinkedIn</a>\")\n",
    "            f.write(\"</div>\")\n",
    "        f.write(\"</body></html>\")\n",
    "\n",
    "def main():\n",
    "    # Replace with your actual list of post URLs\n",
    "    # urls = pd.read_csv(\"linkedin_urls.csv\")[\"url\"].tolist()\n",
    "    urls = ['https://www.linkedin.com/feed/update/urn:li:activity:7353310347857969153?collapsed=1']\n",
    "\n",
    "    driver = init_driver()\n",
    "    all_posts = []\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        try:\n",
    "            post = extract_post_details(driver, url)\n",
    "            all_posts.append(post)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    generate_html(all_posts)\n",
    "    print(f\"✅ Scraping complete. HTML output saved to 'linkedin_posts.html'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
